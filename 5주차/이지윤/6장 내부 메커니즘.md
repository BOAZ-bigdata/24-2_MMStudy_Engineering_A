# 6장 내부 메커니즘

태그: 카프카 핵심 가이드
주차: 4

- 카프카의 내부 작동 방식을 알고 있으면
    - 트러블 슈팅
    - 실행되는 방식 이해하는데 도움
- 카프카 컨트롤러
- 카프타 복제가 작동하는 방식
- 카프카가 프로듀서와 컨슈머의 요청을 처리하는 방식
- 카프카가 저장을 처리하는 방식


## 1. 클러스터 멤버십

- 카프카는 현재 클러스터의 멤버인 브로커들의 목록을 유지하기 위해 아파치 주키퍼 사용
    - 브로커 프로세스는 시작될 때마다 주키퍼에 Ephemeral 노드의 형태로 ID등록
    - 주키퍼의 brokers/ids 경로 구독 → 브로커 추가, 제거 시 알림
    - 동일한 ID를 가진 다른 브로커 시작 시 에러 발생
- 브로커와 주키퍼 간의 연결이 끊어질 경우, 브로커가 시작될 때 생성한 Ephemeral 노드는 자동으로 주키퍼에서 삭제됨
- 브로커 정지 시, 브로커를 나타내는 ZNode 역시 삭제, 브로커의 ID는 다른 자료구조에 남아있게 됨


## 2. 컨트롤러

- 컨트롤러는 일반적인 카프카 브로커의 기능에 더해서 파티션 리더를 선출하는 역할 추가적으로 맡음
    - 가장 먼저 시작되는 브로커는 주키퍼의 /controller 에 Ephemeral 노드를 생성함으로써 컨트롤러가 됨
    - 컨트롤러 노드에 변동 생겼을 때 → 노드에 와치 설정
    - 컨트롤러 브로커가 멈추거나, 주키퍼와의 연결 끊어질 경우, 이 Ephemeral 노드 삭제됨
        - 삭제 시 주키퍼에 컨트롤러 노드 생성 시도
        - 새로운 컨트롤러 선출 시, 주키퍼의 조건적 증가 conditional increment 연산에 의해 증가된 에포크 epoch 값 전달
        - 더 낮은(예전) 에포크 값 전달 받을 시, 컨트롤러부터 메세지 받을 경우 무시
    - 좀비 컨트롤러
        - 새로운 컨트롤러 선출된 지 모른 채 브로커에게 메세지 전달
    - 브로커가 컨트롤러가 되면, 클러스터 메타데이터 관리와 리더 선출을 시작하기 전에 먼저 주키퍼로부터 최신 레플리카 상태 맵 읽어옴 (비동기 API)
    - 브로커가 클러스터를 나갔다는 사실을 컨트롤러가 알아차리면, 컨트롤러는 해당 브로커가 리더를 맡고 있었던 모든 파티션에 대해 새로운 브로커를 할당
- 컨트롤러는 브로커가 클러스터에 추가되거나 제거될 때 파티션과 레플리카 중에서 리더 선출 책임

### KRaft: 카프카의 새로운 래프트 기반 컨트롤러

- 주키퍼 기반 컨트롤러 → 래프트 raft 기반 컨트롤러 쿼럼 이동
- 교체 결정 이유
    - 컨트롤러가 주키퍼에 메타데이터를 쓰는 작업은 동기적, 브로커 메세지 보내는 작업&주키퍼로부터 업데이트 받는 과정 비동기적 → 불일치 발생
    - 컨트롤 재시작 시, 주키퍼로부터 모든 브로커와 파티션에 대한 메타데이터 읽어온 뒤, 모든 브로커로 전송 필요 → 병목
    - 메타데이터 소유권 관련 내부 아키텍처 문제: 어떤 작업 컨트롤러, 어떤 작업은 주키퍼
    - 분산 시스템에 대한 선수 지식 필요
- 새로운 컨트롤러 설계의 핵심 아이디어
    - 카프카가 그 자체에 사용자 상태를 이벤트 스트림을 나타낼 수 있도록 하는 로그 기반 아키텍처 도입
        - 다수의 컨슈머를 사용해서 이벤트를 재생 replay → 최신 상태 업데이트
        - 로그: 이벤트 사이 명확한 순서 부여, 컨슈머들이 하나의 타임을 따라 움직이도록 보장
- 래프트 알고리즘
    - 컨트롤러 노드들은 외부 시스템에 의존하지 않고, 자체적으로 리더 선출 가능
    - 액티브 컨트롤러: 메타데이터 로그의 리더 역할을 맡고 있는 컨트롤러. 브로커가 보내온 모든 RPC 호출 처리
    - 팔로워 컨트롤러: 액티브 컨트롤러에 쓰여진 데이터 복제, 액티브 컨트롤러에 장애 발생 시 즉시 투입될 수 있도록 준비 상태 유지
    - → 모든 컨트롤러는 최신 상태
    - → 기나긴 리로드 기간 필요X
- 컨트롤러 쿼럼 quorum
    - 브로커 프로세스 시작 시 주키퍼가 아닌, 컨트롤러 쿼럼에 등록
    - 운영자가 해제하지 않는 한 유지
    - → 브로커 종료 시, =오프라인 상태, 등록 유지
- 기존 주피커와 직접 통신하던 모든 클라이언트, 블로커 작업들은 컨트롤러로 보내짐


## 3. 복제

- 중요: 개별적인 노드에 필연적으로 장애 발생 → 복제를 통해 신뢰성과 지속성 보장
- 저장된 데이터
    - 토픽을 단위로 해서 조직화
    - 토픽은 1개 이상의 파티션으로 분할
    - 각 파티션은 다수의 레플리카 가질 수 있음
    - 각각의 레플리카는 브로커에 저장, 하나의 브로커는 수백 개~ 수천 개 레플리카 저장
- 리더 레플리카
    - 각 파티션에 리더 역할하는 레플리카 하나씩
    - 일관성 보장 → 모든 쓰기 요청은 리더 레플리카로 주어짐
    - 어느 팔로워 레플리카가 리더 레플리카의 최신 상태 유지하는지 확인
        - 인-싱크 레플리카, 아웃-오브-싱크 레플리카
    - 선호 리더: 토픽 처음 생성 시 리더 레플리카였던 레플리카
        - 파티션 처음 생성되는 시점에서 리더 레플리카가 모든 브로커에 걸쳐 균등하게 분포
- 팔로워 레플리카
    - 파티션에 속한 모든 레플리카 중 리더 레플리카를 제외한 나머지
    - 팔로워는 클라이언트 요청 처리 불가
    - 리더 레플리카로 들어온 최근 메세지 복제 → 최신 상태 유지
    - 리더 레클리카 크래쉬 → 팔로워 레플리카 중 하나가 리더로 승격

## 4. 요청 처리

- 카프카 브로커가 하는 일의 대부분은 클라이언트, 파티션 레플리카, 컨트롤러가 파티션 리더에게 보내는 요청 처리
    - TCP 이진 프로토콜
- 브로커는 연결을 받는 각 포트별로 억셉터acceptor 스레드 하나씩 실행
    - 억셉터 스레드는 연결을 생성하고 들어온 요청을 프로세서 processor 스레드에 넘겨 처리
- 쓰기 요청&읽기 요청
    - → 파티션의 리더 레플리카로 전송
- 어드민 요청
- 메타데이터 요청
    - 카프카 클라이언트가 사용
    - 아무 브로커에 전송 (메타 데이터 캐시 포함)
- 프로토컬 진화
    - 새로운 요청 유형 추가
    - 새로운 기능 추가
    

## 5. 물리적 저장소

- 카프카의 기본 저장 단위는 파티션 레플리카
    - 파티션은 서로 다른 브로커들  사이에 분리 불가
    - 같은 브로커의 서로 다른 디스크에 분할 저장되는 것조차 불가능
    - → 파티션의 크기는 특정 마운트 지점에 사용 가능한 공간에 제한
- 카프카가 데이터를 저장하기 위해 사용 가능한 디렉토리를 어떻게 활용

### 계층화된 저장소

- 현재 대량의 데이터를 저장할 때의 문제
    - 파티션별로 저장 가능한 데이터는 한도 있음
    - 디스크와 클러스터 크기는 저장소 요구 조건에 의해 결정
    - 클러스터의 크기를 키우거나 줄일 때, 파티션 위치를 다른 브로커로 옮기는 데 걸리는 시간은 파티션의 수에 따라 결정
- 계층화된 저장소: 카프카 클러스터의 저장소를 로컬과 원격 (두 계층)
    - 로컬 계층
        - 현재 카프카 저장소 계층과 동일하게 로컬 세그먼트 저장하기 위해
        - 카프카나 브로커의 로컬 디스크 사용
    - 원격 계층
        - 완료된 로그 세그먼트 저장하기 위해
        - HDFS, S3 와 같은 전용 저장소 시스템
- 카프카 클러스터의 메모리와 CPU에 상관없이 저장소 확장 가능

### 파티션 할당

- 사용자가 토픽 생성, 카프카는 우선 이 파티션을 브로커 중 하나에 할당
- 파티션 할당 목표
    - 레플리카들을 가능한 브로커 간에 고르게 분산
    - 각 파티션에 대해 각각의 레플리카는 서로 다른 브로커에 배치되도록 한다.
    - 가능한 각 파티션의 레클리카들을 서로 다른 랙에 할당
- 임의의 브로커부터 시작해서 각 브로커에 라운드 로빈 방식으로 파티션 할당하여 리더 결정

### 파일 관리

- 카프카 운영자는 각각의 토픽에 대한 보존 기한 설정 가능
- 하나의 파티션을 여러 개의 세그먼트로 분할
    - 카프카가 파티션 단위로 메세지를 쓰는 만큼 각 세그먼트 한도가 다 차면 세그먼트를 닫고 새 세그먼트 생성
    - 액티브 세그먼트: 현재 쓰여지고 있는 세그먼트

### 파일 형식

- 각 세그먼트는 하나의 데이터 파일 형태로 저장
- 파일 안에는 카프카의 메세지와 오프셋이 저장됨
- 디스크에 저장되는 데이터의 형식은 사용자가 프로듀서를 통해서 브로커로 보내는, 브로커로부터 컨슈머로 보내지는 메세지 형식과 동일
- 사용자 페이로드 payload, 시스템 헤더
- 메세지 배치: 카프카 프로듀서는 언제나 메세지를 배치 단위로 전송
    - 프로듀서에서 압축 기능 사용할 경우 더 큰 배치 전송할수록 네트워크를 통해 전송, 더 잘 압축됨
- 컨트롤 배치

### 인덱스

- 카프카는 컨슈머가 임의의 사용 가능한 오프셋에서부터 메세지를 읽어오기 시작 가능
    - → 브로커는 오프셋을 활용해 메세지가 저장된 위치를 빠르게 찾아 해당 오프셋부터 메세지 읽기 가능
    - 브로커가 주어진 오프셋의 메세지를 빠르게 찾을 수 있도록 하기 위해, 카프카는 각 파티션에 대한 오프셋 유지
- 카프카는 타임스탬프와 메세지 오프셋에 매핑하는 또 다른 인덱스 가지고 있음
    - 타임스탬프 기준으로 메세지 찾을 때 사용됨
- 인덱스는 세그먼트 단위로 분할됨

### 압착

- 카프카 두 가지 보존 정책
    - 삭제 보존 정책: 지정된 보존 기한보다 더 오래 된 이벤트 삭제
    - 압착 compact 보존 정책: 토픽에서 각 키의 가장 최근값만 저장
        - 압착 설정
    - 두 개를 동시에 적용하도록 설정할 수도 있음
- 클린: 압착된 적이 있었던 메세지들 저장
- 더티: 마지막 압착 작업 이후 쓰여진 메세지들 저장
- 각 브로커는 압착 매니저 스레드와 함께 다수의 압착 스레드 시작
- → 각 스레드는 전체 파티션 크기 대비 더티 메세지의 비율이 가장 높은 파티션 골라서 압착 → 클린

### 삭제된 이벤트

- 특정 키를 갖는 모든 메세지 삭제
    - 해당 키값과 null 밸류값 갖는 메세지 사용
    - 클리너 스레드가 발견 → 압착 & null 밸류 값 메세지 보존→ 특별한 메세지 툼스톤 보존
    - → 시간 지나고서, 클리너 스레드가 툼스톤 메세지 삭제 → 키 완전 삭제
- 어드민 클라이언트에 deleteRecords 메서드

### 토픽은 언제 압착되는가?

- 삭제 정책이 현재의 액티브 세그먼트를 절대 삭제하지 않는 것과 유사하게,
- 압착 정책도 현재의 액티브 세그먼트 절대 압착 X
- 두 설정 매개변수 사용 → 압착 시점 조절 가능
