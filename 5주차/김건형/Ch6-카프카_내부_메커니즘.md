# 카프카 내부 메커니즘

## 클러스터 멤버십

- 카프카는 클러스터의 멤버인 브로커들의 목록을 유지하기 위해 `아파치 주키퍼`를 사용.
- 브로커 프로세스는 시작될 때마다 주키퍼에 `Ephemeral 노드`의 형태로 ID 등록
	- 동일한 ID를 가진 다른 브로커를 시작하면 에러 발생
- 브로커와 주키퍼 간의 연결이 끊어지면, `Ephemeral 노드`는 자동으로 주키퍼에서 삭제됨
	- 브로커 목록을 지켜보고 있던 카프카 컴포넌트들은 브로커가 내려갔음을 알아차림
	- 브로커를 나타내는 `ZNode`도 삭제됨
		- 브로커의 ID는 다른 자료구조에 남음

## 컨트롤러

- 일반적인 카프카 브로커의 기능에 더해서 추가적으로 파티션 리더 선출
- 클러스터에서 가장 먼저 시작되는 브로커는 주키퍼의 `/controller`에 `Ephemeral 노드`를 생성함으로써 컨트롤러가 됨
	- 다른 브로커들도 시작할 때 `/controller`에 노드를 생성하려고 하지만, `노드가 이미 존재함` 예외를 받기 때문에 컨트롤라가 존재함을 앎
	- 브로커들은 주키퍼 컨트롤러 노드의 변동에 대한 알림을 받기 위해 해당 노드에 `와치`를 설정
		- 클러스터 안에 단 한 개의 컨트롤러만 있도록 보장 

- 브로커가 컨트롤러가 되었을 때
	- 클러스터 메타데이터 관리와 리더 선출을 시작하기 전에 주키퍼로부터 최신 레플리카 상태 맵 읽어옴
		- 비동기 API를 사용해서 수행되는 적재 작업
		- 지연을 줄이기 위해 읽기 요청을 여러 단계로 나누어 주키퍼로 보냄

- 브로커가 클러스터를 나갔다는 사실을 컨트롤러가 알아차렸을 때
	- 해당 브로커가 리더를 맡고 있던 모든 파티션에 대해 새로운 브로커 할당
	- 새로운 리더가 필요한 모든 파티션을 순회해 가며 새로운 리더가 될 브로커 결정
		- 해당 파티션의 레플리카 목록에서 바로 다음 레플리카가 새 브로커가 됨
	- 새로운 상태를 주키퍼에 쓴 뒤 새로 리더가 할당된 파티션의 레플리카를 포함하는 모든 브로커에 `LeaderAndISR` 요청 보냄
		- 파티션들에 대한 새로운 리더와 팔로워 정보를 포함하는 요청
		- 효율성을 위해 배치 단위로 묶여 전송

### Kfraft

> - 카프카의 새로운 래프트 기반 컨트롤러 [주피터 기반 컨트롤러로부터 탈피]
> - 3.3 버전부터 프로덕션 환경에서 사용 가능한 기능이 됨

 - Kraft로의 교체 이유
	- 컨트롤러가 주키퍼에 메타데이터를 쓰는 작업은 동기적
	 - 브로커 메시지를 보내는 작업은 비동기적
	 - 주키퍼로부터 업데이트를 받는 과정은 비동기적
	 	 ***브로커, 컨트롤러, 주키퍼 간에 메타데이터 불일치가 발생할 수도 있음***
	 - 컨트롤러가 재시작될 때마다 주키퍼로부터 모든 브로커와 파티션에 대한 메타데이터를 읽어와야 함
		 ***병목. 파티션과 브로커의 수가 증가함에 따라 컨트롤러 재시작은 더더욱 느려짐***	 
	 - 두 개의 분산 시스템 (주키퍼, 카프카)에 대한 학습 부담

#### 기능

- 컨트롤러 선출
	- 로그 기반 아키텍처
		- 카프카 자체에 사용자가 상태를이벤트 스트림으로 나타낼 수 있음
		- 다수의 컨슈머를 사용해서 이벤트를 재생함으로써 최신 상태를 빠르게 따라잡을 수 있음
		- 컨슈머들은 항상 하나의 타임라인을 따라 움직임
- 클러스터 메타데이터 저장
	[*운영중인 브로커, 설정, 토픽, 파티션, 레플리카 관련 정보, 컨트롤러 자체가 관리하는 메타데이터*]

	- 컨트롤러 노드들은 메타데이터 이벤트 로그를 관리하는 래프트 쿼럼이 됨
		- 해당 로그는 클러스터 메타데이터의 변경 내역을 저장
			[*주키퍼에 저장되어 있는 모든 정보들 (토픽, 파티션, ISR, 설정 등)*]

- 래프트 알고리즘을 통해 컨트롤러 노드들은 외부 시스템에 의존하지 않고 자체적으로 리더 선출이 가능해짐

- **액티브 컨트롤러** : 메타데이터 로그의 리더 역할을 맡고 있는 컨트롤러
	- 브로커가 보내온 모든 RPC 호출 처리
- **팔로워 컨트롤러**
	- 액티브 컨트롤러에 쓰여진 데이터들 복제
	- 액티브 컨트롤러에 장애 발생 시 즉시 투입될 수 있는 준비 상태 유지
		- 모든 컨트롤러가 최신 상태를 가지고 있으므로 컨트롤러 장애 복구에서 모든 상태를 새 컨트롤러로 이전하는 `Reload Period`를 필요로 하지 않음

---

## 복제

- 개별적인 노드에는 필연적으로 장애가 발생할 수밖에 없기 때문에, 카프카의 신뢰성과 지속성을 보장하기 위한 핵심적인 요소

- 카프카에 저장되는 데이터는 토픽을 단위로 하여 조직화 됨
	- 각 토픽은 한 개 이상의 파티션으로 분할
		- 각 파티션은 다수의 레플리카를 가질 수 있음
			- 각 레플리카는 브로커에 저장
			- 하나의 브로커는 수백 ~ 수천 개의 레플리카 저장

### 레플리카 종류

- 리더 레플리카
	- 리더 역할
	- 모든 쓰기 요청은 리더 레플리카로 주어짐
	- 클라이언트는 리더 레플리카 또는 팔로워로부터 레코드 읽어올 수 있음
- 팔로워 레플리카
	- 모든 레플리카 중에서 리더 레플리카를 제외한 나머지
	- 팔로워는 클라이언트의 요청을 처리할 수 없음
	- 주요 역할
		- 리더 레플리카로 들어온 최근 메시지들을 복제하여 최신 상태 유지
			- 동기화 유지를 위해 리더 레플리카에 읽기 요청 보냄
			- 리더 레플리카는 메시지를 포함하여 메시지 오프셋을 돌려 줌
				- 리더는 팔로워가 얼마나 뒤쳐져 있는지 알 수 있음
				- 만약 팔로워가 일정 시간 이상 읽기 요청을 보내지 않거나, 가장 최근에 추가된 메시지를 따라잡지 못하는 경우 해당 레플리카는 `out-of-sync replica`로 간주
				- 지속적으로 최신 메시지를 요청하고 있는 레플리카는 `in-sync replica`
		- 해당 파티션의 리더 레플리카에 충돌이 날 경우, 이 중 하나가 새 리더 레플리카로 승격

- 선호 리더 [preferred leader]
	- 토픽이 처음 생성되었을 때 리더 레플리카였던 레플리카
		- 파티션이 처음 생성되는 시점에는 리더 레플리카가 모든 브로커에 걸쳐 균등하게 분포됨
			- 선호 리더가 실제 리더가 되면 부하가 브로커 사이에 균등하게 분배될 것으로 예상 가능

> - **팔로워로부터 읽기**
> 	- 클라이언트가 리더 레플리카 대신 가장 가까이에 있는 인-싱크 레플리카로부터 읽게 하는 것
> 		- 네트워크 트래픽 비용 줄일 수 있음
> 	- 커밋된 메시지만 읽도록 하여 팔로워로부터 메시지를 읽을 때도 신뢰성 보장
> 		- 모든 레플리카가 리더가 어느 메시지까지를 커밋했는지 알아야 함
> 		- 리더는 팔로워로 보내는 데이터에 현재의 `하이 워터마크(마지막으로 커밋된 오프셋)` 포함시킴
> 			- 하이 워터마크 전파에 시간이 소요되기 때문에, 팔로워로부터 읽는 것은 약간의 지연이 발생

---
## 요청 처리

- 카프카는 언제나 클라이언트가 연결을 시작하고 요청을 전송, 브로커는 요청을 처리하고 클라이언트로 응답을 보냄
	- 특정 클라이언트가 브로커로 전송한 모든 요청은 브로커가 받은 순서대로 처리
		- 카프카가 저장하는 메시지는 순서가 보장되며, 메시지 큐로도 활용될 수 있음

- 브로커는 연결을 받는 각 포트별로 `Acceptor 스레드`를 하나씩 실행시킴
- `Acceptor 스레드`는 연결을 생성하고 들어온 요청을 `Processor 스레드(Network 스레드)`에 넘겨 처리
	- 클라이언트 연결로부터 들어온 요청들을 받아 요청 큐에 넣고, 응답 큐에서 응답을 가져다 클라이언트로 보냄
- 요청이 요청 큐에 들어오면, `I/O 스레드`가 요청을 가져와서 처리
	- 쓰기 요청
		- 카프카 브로커로 메시지를 쓰고 있는 프로듀서가 보낸 요청
	- 읽기 요청
		- 카프카 브로커로부터 메시지를 읽어오고 있는 컨슈머나 팔로워 레플리카가 보낸 요청
	- 어드민 요청
		- 토픽 생성, 삭제와 같이 메타데이터 작업을 수행중인 어드민 클라이언트가 보낸 요청
	- 메타데이터 요청
		- 클라이언트가 다루고자 하는 토픽들의 목록을 포함
		- 서버는 토픽들에 존재하는 파티션, 파티션의 레플리카, 어떤 레플리카가 리더인지를 명시하는 응답 반환
		- 메타데이터 요청은 아무 브로커에 보내도 상관 없음
		- 클라이언트는 이 정보를 캐시해 두고, 이 정보를 사용해서 각 파티션의 리더 역할을 맡고 있는 브로커에 바로 쓰거나 읽음
		- 클라이언트가 요청에 대해 `Not a Leader`에러를 리턴받을 경우, 요청 재시도 전에 메타데이터를 먼저 새로고침

---
## 물리적 저장소

- 카프카의 기본 저장 단위는 파티션 레플리카
- 파티션은 서로 다른 브로커들 사이에 분리될 수 없음
	- 같은 브로커의 서로 다른 디스크에 분할 저장도 X
- 파티션의 크기는 특정 마운트 지점에 사용 가능한 공간에 제한 받음
- 카프카를 설정할 때는 파티션들이 저장될 디렉토리 목록을 `log.dirs`매개변수에 지정
	- `log4j.properties` 파일에서 정의되는, 에러 로그 등이 저장되는 디렉토리와는 다름

### 계층화된 저장소

- 카프카는 대량의 데이터를 저장하기 위한 목적으로 사용되고 있기 때문에 생기는 문제가 있음
	- 파티션 별로 저장 가능한 데이터에 한도가 있음
	- 디스크와 클러스터 크기는 저장소 요구 조건에 의해 결정됨
		- 지연과 처리량이 주 고려사항인 경우 비용으로 직결

계층화된 저장소
- 로컬 계층
	- 현재의 카프카 저장소 계층과 똑같이 로컬 세그먼트를 저장하기 위해 카프카 브로커의 로컬 디스크 사용
- 원격 계층
	- 완료된 로그 세그먼트를 저장하기 위해 HDFS나 S3 같은 전용 저장소 시스템 사용

> 로컬 저장소가 리모트 계층 저장소에 비해 비싼 것이 일반적이므로, 로컬 계층의 보존 기한은 몇 시간 이하, 원격 계층의 보존 기한은 며칠 또는 몇 달로 설정하는 게 일반적

- 지연에 민감한 애플리케이션은 로컬 계층에 있는 최신 레코드를 읽어오고,
- 빠진 처리 결과를 메꾸는 작업 / 장애 복구 중인 애플리케이션은 오래된 데이터가 필요하므로 원격 계층의 데이터 전달

> 계층화된 저장소 기능의 이중화된 구조 덕분에, 카프카 클러스터의 메모리, CPU에 상관없이 저장소 확장 가능
> 	> 장기간용 저장 솔루션으로서의 역할 가능해짐

### 파티션 할당

1. 사용자가 토픽 생성
2. 카프카는 이 파티션을 브로커 중 하나에 할당
	- 브로커 6개, 파티션 10개, 복제 팩터가 3인 토픽을 생성할 때, 카프카는 30개의 파티션 레플리카를 6개의 브로커에 할당해야 함
		- 레플리카들을 가능한 한 브로거 간 고르게 분산시켜야 함
			- 브로커 별로 5개의 레플리카 할당
		- 각 파티션에 대해 각각의 레플리카는 서로 다른 브로커에 배치
			- 파티션 0의 리더가 브로커 2에 있다면, 팔로워들은 브로커 3, 4에 배치
		- 브로커에 랙 정보가 설정되어 있다면, 가능한 한 각 파티션의 레플리카들을 서로 다른 랙에 할당
	- 임의의 브로커부터 시작해서 각 브로커에 라운드 로빈 방식으로 파티션 할당하여 리더 결정

## 인덱스

- 컨슈머가 오프셋 100부터 시작되는 1MB의 메시지를 요청할 경우, 브로커는 오프셋 100에 해당하는 메시지가 저장된 위치를 빠르게 찾아서, 해당 오프셋에서부터 메시지를 읽기 시작할 수 있어야 함
	- 빠르게 찾기 위해 카프카는 각 파티션에 대해 오프셋을 유지
	- 이 인덱스는 오프셋과 세그먼트 파일 및 그 안에서의 위치를 매핑

## 압착

- 카프카는 설정된 기간 동안만 메시지 저장, 보존 기간이 지나간 메시지들은 삭제
- 고객의 주소와 같은 경우, 특정 기간 보다는 가장 마지막 것만 저장하는 게 더 효율적

### 카프카의 보존 정책

- 삭제 보존 정책
	- 지정된 보존 기한보다 더 오래된 이벤트들 삭제
- 압착 보존 정책
	- 토픽에서 각 키의 가장 최근값만 저장하도록 함