# 9장 데이터 파이프라인 구축하기

태그: 카프카 핵심 가이드
주차: 5

- 아파치 카프카를 사용한 데이터 파이프라인 구축 활용 사례
    - 아파치 카프카가 두 개의 엔드포인트 중 하나가 되는 데이터 파이프라인 구축
    - 두 개의 서로 다른 시스템을 연결하는 파이프라인을 만들면서 그 중간에 카프카를 사용하는 경우
- 데이터 파이프라인에 있어서 카프카가 갖는 주요한 역할: 데이터 파이프라인의 다양한 단계 사이에 있어 매우 크고 안정적인 버퍼 역할을 해 줄 수 있다는 점

## 1. 데이터 파이프라인 구축 시 고려사항

### 적시성

- 좋은 데이터 통합 시스템: 각각의 데이터 파이프라인에 대해 서로 다른 적시성 요구 조건 지원
- 업무에 대한 요구 조건 변경 시, 이전하기 용이
- 카프카는 확장성과 신뢰성을 보유한 저장소를 갖춘 스트리밍 플랫폼
    - 거의 실시간으로 작동하는 데이터 파이프라인에서부터 일 단위로 작동하는 배치 작업에서도 사용 가능
    - 카프카 = 쓰는 쪽과 읽는 쪽 사이의 시간적 민감도에 대한 요구 조건 분리시키는 거대한 버퍼
    - 백프레셔 적용

### 신뢰성

- 모든 종류의 장애 발생에 대해 신속하고 자동화된 복구 수행 필요
- 전달 보장 delivery guarantee
- 카프카는 자체적으로 “최소 한 번” 전달 보장, 트랜잭션 모델이나 고유 키 지원하는 외부 데이터 저장소 결합 시, “정확한 한 번” 보장 가능

### 높으면서도 조정 가능한 처리율

- 모던 데이터 시스템 modern data system 에서 자주 요구되는 것과 같이, 매우높은 처리율을 가질 수 있도록 확장 가능 필요
    - 처리율이 갑자기 증가해야 하는 경우에도 적응 필요
- 카프카 = 쓰는 쪽과 읽는 쪽  사이의 버퍼 역할
    - → = 프로듀서의 처리율과 컨슈머의 처리율을 묶어서 고려 X
- 카프카 = 높은 처리율을 받아낼 수 있는 분산 시스템
    - 평범한 클러스터에서도 초당 수백 메가바이트 처리 가능
    - 여러 종류의 압축 코덱 지원 → 사용자와 운영자가 스토리지 자원의 사용 제어 가능

### 데이터 형식

- 서로 다른 데이터 형식과 자료형을 적절히 사용
- 서로 다른 데이터베이스와 다른 저장 시스템마다 지원되는 자료형은 다름
- 카프카 자체와 커넥트 API는 데이터 형식에 완전히 독립적
    - 프로듀서와 컨슈머는 필요한 데이터 형식을 지원할 수만 있다면 어떤 시리얼라이저도 사용 가능
    - 카프카 커넥터는 자료형과 스키마를 포함하는 고유한 인메모리 객체 가지고 있는데
        - 어떠한 형식으로도 저장 가능할 수 있도록 하는 장착 가능한 컨버터 지원
        - → 데이터 형식이 사용 커넥터 영향 X
    - 많은 소스와 싱크는 스키마 가짐

### 변환

- 데이터 파이프라인을 구축하는 방식: ETL, ELT
    - ETL: 추출 - 변환 - 적재
        - 데이터 파이프라인이 통과하는 데이터에 변경을 가하는 작업까지 담당
        - 장점: 데이터 수정 뒤 다시 저장 X → 시간과 공간 절약
        - 단점: 파이프라인에서 데이터의 변환이 일어나기 때문에, 파이프라인의 하단에서 데이터를 처리하고자 할 경우 손쓸 방법X
    - ELT: 추출 - 적재 - 변환
        - 데이터 파이프라인은 대상 시스템에 전달되는 데이터가 원본 데이터와 최대한 비슷하도록 최소환의 변환만 수행
        - 장점: 대상 시스템이 가공되지 않은 raw data를 받아서 모든 필요한 처리 진행 → 사용자에게 최대한 유연성 제공
        - 단점: 변환 작업이 대상 시스템의 CPU와 자원 잡아먹음
- 카프카 커넥트는 원본 시스템의 데이터를 카프카로 옮길 때 혹은 카프카의 데이터를 대상 시스템으로 옮길 때
    - 단위 레코드 변환해주는 단일 메세지 변환 기능 탑재
    - 다른 토픽으로 메세지 보내거나, 필터링, 자료형 변경, 특정 필드 삭제 등의 기능 포함
    - 조인, 집적과 같이 더 복잡한 변환 작업 → 카프카 스트림 사용해 처리
        - 집적: 여러 가지 것을 한데 모아 쌓거나 누적하는 것을 의미 ≠ 집계

### 보안

- 카프카는 소스에서 카프카로 데이터를 보내거나, 카프카에서 싱크로 데이터를 보내는 데이터 전송 과정에서 데이터 암호화 지원
- 카프카는 허가받거나 허가받지 않은 접근 내역 추적하는 감사 로그 지원
- 카프카 커넥트와 커넥터는 외부 데이터 시스템에 연결하고 인증 가능해야 함
    - 커텍터도 외부 데이터 시스템의 인증 통과할 수 있도록 자격 증명 credential 포함 필요
    - 자격 증명을 설정 파일에 포함하는 것은 권장 X
    - → 하시코프 볼트와 같은 외부 비밀 관리 시스템 사용
    - 카프카 커넥트는 외부 비밀 설정 지원

### 장애 처리

- 카프카는 모든 이벤트를 장기간에 걸쳐 저장하도록 설정 가능
- → 필요한 경우 이전 시점으로 돌아가서 에러 복구 가능

### 결합 Coupling과 민첩성 Agility

- 데이터 파이프라인을 구현할 때 중요한 것은 데이터 원본과 대상을 분리할 수 있어야 함
- 의도치 않게 결합이 생기는 경우
    - 임기응변 Ad-hoc 파이프라인
    - 메타데이터 유실
    - 과도한 처리

## 2. 카프카 커넥트 vs. 프로듀서/컨슈머

- 카프카 커넥트: 카프카를 직접 코드나 API를 작성하지 않았고, 변경도 할 수 없는 데이터 저장소에 연결시켜야 할 때 사용
    - 외부 데이터 저장소의 데이터를 카프카로 가져 오거나
    - 카프카에 저장된 데이터를 외부 저장소로 내보낼 수 있음
    - 카프카 커넥터를 사용하려면 연결하고자 하는 데이터 저장소에 맞는 커넥터 필요 → 요즘은 많음 = 설정 파일 작성만 하면 됨
- 커넥트 API
    - 설정 관리, 오프셋 저장, 병렬 처리, 에러 처리, 서로 다른 데이터 형식 지원 및 REST API를 통한 표준한 관리 제공
    - → 카프카 클라이언트보다는 커넥트 API 사용 권장

## 3. 카프카 커넥트

- 카프카와 다른 데이터 저장소 사이에 확장성과 신뢰성을 가지면서 데이터를 주고받을 수 있는 수단 제공
- 커넥터 플러그인을 개발, 실행하기 위한 API와 런타임 제공
    - 커텍터 플러그인: 카프카 커넥트가 실행시키는 라이브러리, 데이터 이동 담당
- 여러 워커 프로세스들의 클러스터 형태로 실행
- 커넥터는 대용량의 데이터 이동을 병렬화해서 처리, 워커의 유휴 자원을 효율적으로 활용하기 위해 태스크를 추가로 실행
    - 소스 커넥터 태스크
    - 싱크 커넥터 태스크

### 카프카 커넥트 실행하기

- 카프카 커넥트를 프로덕션 환경에서 사용할 경우, 카프카 브로커와는 별도의 서버에서 커넥트 실행 필요
    - → 모든 서버에 일단 카프카 설치, 일부에서는 브로커 실행, 나머지에서는 카프카 커넥트 실행
- 워커 클러스터를 싱행한 뒤, 커넥터를 생성 또는 삭제
    - 카프카 커넥트 워커 실행하는 것은 브로커 실행하는 것과 유사

### 커넥터 예제

- 파일 소스와 파일 싱크
- MySQL에서 Elasticsearch 로 데이터 보내기
    - MySQL 테이블 하나의 내용을 카프카 토픽으로 보낸 뒤, 여기서 다시 엘라스틱서치로 보내서 내용물을 인덱싱

### 개별 메세지로 변환

- ETL 파이프라인은 변환 단계가 포함
- SMT: 카프카 생태계에서는 이러한 상태 없는 변환을 상태가 있을 수 있는 스트림 처리와 구분하여 개별 메세지 변환 single message transformation 이라고 부름
    - 카프카 커넥트가 메세지를 복사하는 도중에 데이터 변환 작업의 일부
    - 보통 코드를 작성할 필요 없이 수행

### 카프카 커넥트: 좀 더 자세히 알아보기

- **커넥터와 태스크**
    - 커넥터 플러그인은 커넥터 API를 구현
    - 커넥터 API는 커넥터와 태스크를 포함함

<aside>
💡 커넥터 플러그인, 커넥터 API, 커넥터, 태스크

- **커넥터 플러그인**: Kafka Connect API를 사용하여 구현된 데이터 통합용 소프트웨어입니다.
    - Kafka Connect API를 사용하여 구현된 소프트웨어 컴포넌트입니다. 즉, 특정 데이터 소스(예: MySQL, Elasticsearch 등)와 Kafka 간의 데이터 전송을 처리하는 코드를 포함한 구현체
- **Kafka Connect API**: 커넥터 플러그인을 구현하기 위한 프레임워크이며, 커넥터와 태스크를 포함하는 구조를 제공합니다.
    - 커넥터를 구현하기 위한 프레임워크입니다. 이 API는 데이터 소스와 Kafka, 또는 Kafka와 데이터 싱크(목적지) 간의 데이터를 주고받는 커넥터를 만들 때 사용
- **커넥터**: 특정 데이터 소스나 싱크와의 통합 작업을 관리하는 상위 개념으로, 여러 태스크로 작업을 분할합니다.
    - 특정 데이터 소스나 데이터 싱크와 연결되어 데이터를 읽거나 쓰는 작업을 관리하는 상위 수준의 개념입니다. 커넥터는 전체 데이터 통합 작업을 관리하고, 필요한 만큼의 태스크를 할당하여 병렬 처리를 가능하게 합니다.
- **태스크**: 커넥터가 할당한 작업을 병렬로 처리하는 실행 단위입니다.
    - 커넥터가 처리해야 할 작업을 분할하여 병렬로 수행하는 단위입니다. 하나의 커넥터는 여러 개의 태스크로 나뉘어 작업을 수행할 수 있으며, 이로 인해 데이터 처리 속도를 높일 수 있습니다.
    - 예를 들어, 대용량의 데이터베이스에서 데이터를 가져오는 소스 커넥터는 여러 개의 태스크를 사용하여 데이터를 병렬로 읽어올 수 있습니다.
</aside>

- **워커**
    - 카프카 커넥트의 워커 프로세스
        - 커넥터와 태스크를 실행시키는 역할을 맡는 “컨테이너” 프로세스
        - 커넥터와 그 설정을 정의하는 HTTP 요청을 처리
        - 커텍터 설정을 내부 카프카 토픽에 저장
        - 커텍터와 태스크 실행, 적절한 설정값을 전달
    - 관심사의 분리
        - 커넥터, 태스크와는 서로 다른 책임
            - 커넥터, 태스크: 데이터 통합에서 데이터 이동 단계 맡음
            - 워커: REST API, 설정 관리, 신뢰성, 고가용성, 규모 확장성, 부하 분산 담당
    - **커넥터**와 **태스크**가 실질적으로 데이터를 이동시키는 작업을 수행하는 주체라면, **워커(Worker)**는 그 작업들이 원활하게 수행되도록 전체적인 **관리와 실행 환경**을 제공하는 역할

- **컨버터 및 커넥트 데이터 모델**
    - **데이터 API**: 소스 커넥터에서 데이터를 Kafka로 보내기 위해 사용하는 API.
    - **소스 커넥터**: 외부 데이터 소스에서 데이터를 읽어와 Kafka로 전송하는 역할을 수행하며, 이 과정에서 데이터 API를 사용합니다.
    - **싱크 커넥터**

- **오프셋 관리**
    - 워커 프로세스가 커넥터에 제공하는 편리한 기능 중 하나
    - 커넥터는 어떤 데이터를 이미 처리했는지 알아야함
    - 어느 이벤트가 이미 처리되었는지에 대한 정보 유지 관리
    - 소스 커넥터
        - 커넥터가 커넥트 워커에 리턴하는 레코드에는 논리적인 파티션과 오프셋이 포함됨
        - 소스 커넥터가 레코드들을 리턴
        - → 워커는 이 레코드를 카프카 브로커로 보냄
        - → 레코드를 성공적으로 쓴 뒤, 해당 요청에 대한 응답
        - → 워커는 방금 전 카프카로 보낸 레코드에 대한 오프셋 저장
    - 싱크 커넥터
        - 비슷한 과정을 반대로
        - 토픽, 파티션, 오프셋 식별자가 포함되어 있는 카프카 레코드를 읽은 뒤
        - → 커넥터의 put() 메서드 호출
        - → 레코드를 대상 시스템에 저장
        - → 성공 시 싱크 커넥터는 커넥터에 주어졌던 오프셋을 카프카에 커밋

## 4. 카프카 커넥트의 대안

- 커넥트 API가 제공하는 편의성, 신뢰성은 매력적
- 카프카에 데이터를 집어넣거나 내보내는 방법이 이것뿐인 것은 아님
- 카프카 커넥트의 대안
    - 다른 데이터 저장소를 위한 수집 프레임워크
        - 카프카가 아키텍처의 핵심 부분이면서, 많은 수의 소스와 싱크를 연결하는 것이 목표라면 카프카 커넥트 API 추천
        - 하지만 그 외는 다른 거
    - GUI 기반 ETL 툴
        - 데이터 저장소만 통합하는 ETL 툴의 성공적인 대안
    - 스트림 프로세싱 프레임워크
