# 3주차 정리
- 목차
  - 웹 크롤러 설계
  - 채팅 시스템 설계
    
## 웹 크롤러 설계
- 검색 엔진에서 널리 쓰는 기술
- 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적
- 몇 개 웹 페이지에서 시작하여 그 링크를 따라 나가면서 새로운 콘텐츠를 수집함
  - 검색 엔진 인덱싱
    - 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만듦 ,ex) 구글 검색 엔진
  - 웹 아카이빙
    - 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
  - 웹 마이닝
  - 웹 모니터링
    - 크롤러를 사용해 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링 할 수 있음

### 1.문제 이해 및 설계 범위 확정
- 웹 크롤러의 기본 알고리즘
  1. URL 집합 입력이 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드함
  2. 다운받은 웹 페이지에 URL들을 추출함
  3. 추출된 URL들을 다운로들할 URL 목록에 추가하고, 위의 과정을 처음부터 반복함
 (실제는 더욱 복잡하지만... 대략적으로)

- 웹 크롤러가 만족 시켜야 할 속성
  - 규모 확장성 : 병행성을 활용
  - 안정성 : 비정상적인 입력이나, 환경에 잘 대응해야함
  - 예절(politeness) : 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안됨
  - 확장성 : 새로운 형태의 콘텐츠를 지원하기 쉬워야함

### 2.개략적 설계안 제시 및 동의 구하기

 <img width="691" alt="스크린샷 2024-07-29 오후 12 27 06" src="https://github.com/user-attachments/assets/5726c614-b1d1-488d-88a3-83965afb7e6f">

- 시작 URL 집합
  - 웹 크롤러가 크롤링을 시작하는 출발점
  - 크롤러가 가능한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는것이 바람직함
  - 주제별로 세분화된 다른 URL을 사용함
  - 등등...
- 미수집 URL 저장소
  - 웹 크롤러는 크롤링 상태를 다운로드할 URL 과 다운로드된 URL로 나눔
  - 다운로들할 URL을 저장 관리하는 컴포넌트를 미수집 URL 저장소라고 부름 (FIFO,Queue)
- HTML 다운로더
  - 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
  - 다운로드할 페이지의 URL은 미수집 URL저장소가 제공함
- 도메인 이름 변환기
  - 웹 페이지를 다운받으려면, URL을 IP 주소로 변환하는 절차가 필요함
  - HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP주소를 알아냄
- 콘텐츠 파서
  - 웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 함
  - 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려지기때문에, 독립된 컴포넌트로 만듦
- 중복 콘텐츠
  - 데이터 중복을 줄이고, 데이터 처리에 소용되는 시간을 줄임
  - 웹 페이지의 해시 값을 비교하는 방법 사용
- URL 추출기
  - HTML 페이지를 파싱하여 링크들을 골라내는 역할을 함
- URL 필터
  - 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속시 오류 생기는 URL, deny list에 포함된 URL 등을 크롤링 대상에서 배제함
- 웹 크롤러 작업 흐름
<img width="823" alt="스크린샷 2024-07-29 오후 12 47 29" src="https://github.com/user-attachments/assets/6c2fcf59-68e7-42b9-8b5e-87b98fae1a0f">

### 3.상세 설계

- DFS vs BFS
  - 웹은 directed graph, 페이지는 node, 하이퍼링크는 edge 라고 보면됨
  - 크롤링 프로세스는 이런 그래프를 탐색하는 과정
  - 그래프 크기가 클 경우, 어느 정도로 깊숙이 가게 될지 가늠이 어렵기때문에, DFS는 좋은 선택이 아닐 수 있음
  - 주로 BFS(FIFO 큐 알고리즘)를 사용함
  - 이 알고리즘도 문제가 발생함
    - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아감 -> 이때 이 링크들을 병렬로 처리하게되면 서버는 수많은 요청으로 과부하 걸림(예의 없는 크롤러)
    - 표준적 BFS 알고리즘은 URL간에 우선순위를 두지 않음 -> 우선순위를 구별하는것이 필요함
- 미수집 URL 저장소
  - 미수집 URL 저장소를 활용하면 예의를 갖춘 크롤러, URL사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있음
  - 예의
    - 웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것(impolite)을 삼가해야 함
    - 예의 바른 크롤러는 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청하는 것
    - 같은 웹 사이트의 페이지를 다운받는 테스크는 시간차를 두고 실행하도록 하면 됨
    - 이 요구사항을 위해서 웹 사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지하면 됨
      - 각 다운로드 스레드는 별도 FIFO 큐를 가지고 있어서, 해당 큐에서 꺼낸 URL만 다운로드하면 됨
<img width="772" alt="스크린샷 2024-07-29 오후 1 16 21" src="https://github.com/user-attachments/assets/f04386fd-71e6-4697-9472-5ad109020935">

    - 큐 라우터 : 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장함
    - 매핑 테이블 : 호스트 이름과 큐 사이의 관계를 보관하는 테이블
    - FIFO 큐 : 같은 호스트에 속한 URL은 언제나 같은 큐에 보관됨
    - 큐 선택기 : 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할
    - 작업 스레드 : 전달된 URL을 다운로드하는 작업 수행
   
- 우선순위
  - 크롤러 입장에서는 중요한 페이지 먼저 수집하도록 하는 것이 바람직함
  - 다양한 척도 사용 : 페이지랭크, 트래픽 양, 갱신 빈도...
  - 우선순위 계산을 위해 '순위결정장치'를 사용함 -> 큐 선택기에서 순위가 높은 큐에서 더 자주 꺼내지도록 설계
  
<img width="929" alt="스크린샷 2024-07-29 오후 1 27 22" src="https://github.com/user-attachments/assets/b5700bb8-0269-4399-b8de-6d19e94e16aa">

<img width="857" alt="스크린샷 2024-07-29 오후 1 27 30" src="https://github.com/user-attachments/assets/4a728a81-9ac4-4c8f-b362-ed25668c28a0">

- 전면 큐 : 우선순위 결정 과정을 처리함
- 후면 큐 : 크롤러가 예의 바르게 동작하도록 보증함
- 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라도, 주기적으로 재수집을 해줘야함
  - 웹 페이지의 변경 이력 활용
  - 우선순위를 활용해, 중요한 페이지는 좀 더 자주 재수집

- 미수집 URL 저장소를 위한 지속성 저장장치
  - 처리해야할 URL을 모두 메모리에 보관하는것은 바람직하지 않음
  - 대부분의 URL은 디스크에 두지만, IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두는 것임
  - 버퍼가 주기적으로 데이터를 디스크에 기록할 것임

- HTML 다운로더
  - HTTP 프로토콜을 통해 웹 페이지를 내려받음
  - Robots.txt(로봇 제외 프로토콜)
    - 웹 사이트가 크롤러와 소통하는 표준적 방법
    - 크롤러가 수집해도 되는 페이지 목록을 바탕으로 긁어갈 파일들의 규칙을 확인함
  - HTML 다운로더 성능 최적화
    1. 분산 크롤링
       - 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산함
       - URL 공간을 작은 단위로 분할하여, 각 서버는 그중 일부의 다운로드를 담당하도록 함
    2. 도메인 이름 변환 결과 캐시
       - DNS 요청을 보내고 결과를 받는 작업의 동기적 특성이 있음
       - 결과를 받기 전까지는 다음 작업을 진행할 수 없음
       - 따라서, DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고, cron job을 돌려 주기적으로 갱신하도록 설계
    3. 지역성
       - 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법 
    4. 짧은 타임아웃
       - 응답이 느리거나, 응답이 없을때, 대기시간의 최대 시간을 미리 정해두는거
       - 이 시간 동안 서버가 응답하지 않으면, 크롤러는 해당 페이지 다운로드를 중단하고 다음 페이지로 넘어감
    - 안정성
      - 안정 해시 : 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술
      - 크롤링 상태 및 수집 데이터 저장 : 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해둬야함
      - 예외 처리
      - 데이터 검증 : 시스템 오류를 방지하기 위한 중요한 수단
    - 확장성
      - 확장성을 갖도록 설계해야 함
      <img width="834" alt="스크린샷 2024-07-29 오후 1 47 35" src="https://github.com/user-attachments/assets/38c622b7-d263-41dd-82b4-5043112bd802">

- 문제 있는 콘텐츠 감지 및 회피
  1. 중복 콘텐츠
     - 해시나 체크섬을 사용하여 탐지 가능함
  2. 거미 덫
     - 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지
     - 알고리즘을 만들거나, 수작업으로 필터를 해줘서 해결하는 방법이 있음
  3. 데이터 노이즈
     - 가치가 없는 페이지는 제외해야 함
    
### 마무리
- 면접관과 추가로 논의해보면 좋은거..
  - 서버 측 렌더링
  - 원치 않는 페이지 필터링
  - 데이터베이스 다중화 및 샤딩 다중화
  - 수평적 규모 확장성
  - 가용성,일관성,안전성
  - 데이터 분석 솔루션


## 채팅 시스템 설계

